#!/usr/bin/python

# FIXME : Allow reading from a sources.list file, parsing into scheme, server, path, codename and components

params = {}


import os , sys


import repolib

if sys.argv[1:] :
    if len(sys.argv) > 2 :
        print "Too many arguments"
        print "Usage : %s repo_name" % os.path.basename( sys.argv[0] )
        sys.exit(2)
    repo_name = sys.argv[1]
else :
    print "Usage : %s repo_name" % os.path.basename( sys.argv[0] )
    sys.exit(1)

try :
    repo = repolib.MirrorRepository.new( repo_name )
except Exception , ex :
    print "Exception : %s" % ex
    sys.exit(255)

if not os.path.isdir( repo.destdir ) :
    print "Destination directory '%s' does not exits" % repo.destdir
    sys.exit(2)


meta_files = repo.get_master_file( params )
print "HELLO META" , meta_files
repolib.logger.info( "HELLO META %s" % meta_files )

if meta_files.values().count( False ) == len(meta_files) :
    repolib.logger.critical( "No valid metadata files found, exiting" )
    sys.exit(255)

if meta_files.get('') is True :
    repolib.logger.info( "Updated repository, no processing required" )
    sys.exit(0)

# After verify all the mirroring parameters, it is safe to create directory tree

suite_path = repo.repo_path()
print "SUITE :",suite_path
print "       ",repo.get_subrepos()
for subrepo in repo.get_subrepos() :
    print "HOLANDO", subrepo.metadata_path(False)

repo.build_local_tree()
print "BUILD TREE FOR",repo

# Once created, we move in the primary metadata file

local_repodata = repo.write_master_file( meta_files )
print "HELLO LOCAL" , local_repodata
repolib.logger.info( "HELLO LOCAL %s" % local_repodata )

repolib.logger.setLevel( repolib.logging.INFO )

map( repolib.logger.info , repo.info( local_repodata ).split("\n") )


download_pkgs = repo.get_download_list()
print "REANDO",download_pkgs,repo
download_size = 0
missing_pkgs = []

packages = {}
print "ENTER WITH", repo.get_subrepos() 
for subrepo in repo.get_subrepos() :

    repolib.logger.info( "Scanning %s" % ( subrepo , ) )

    print "CHECING FOR",subrepo, local_repodata , params
    fd = subrepo.check_packages_file( local_repodata , params )
    if fd is True :
        continue

    _size , _pkgs , _missing = subrepo.get_package_list( fd , params , repo.filters )
    download_size += _size
    download_pkgs.queue( _pkgs )
    missing_pkgs.extend( _missing )


_size = download_size / 1024 / 1024
if _size > 2048 :
    repolib.logger.info( "Total size to download : %.1f Gb" % ( _size / 1024 ) )
else :
    repolib.logger.info( "Total size to download : %.1f Mb" % _size )

if missing_pkgs :

    _missing = {}
    for pkg in missing_pkgs :
        _missing[ pkg ] = 1

    found_pkgs = {}
    for pkg in download_pkgs :
        # NOTE : We don't break the loop to ensure we get all the available archs
        if pkg['name'] in _missing.keys() :
            # repolib.logger.info( "FOUND %s" % pkg['name'] )
            found_pkgs[ pkg['name'] ] = pkg
    # NOTE : extending here is safer
    download_pkgs.queue( found_pkgs.values() )

    missing = []
    for pkg in _missing.keys() :
        if not pkg in found_pkgs.keys() :
            missing.append( pkg )
    if missing :
        repolib.logger.warning( "There are %d missing requirements : %s " % ( len(missing) , missing[:5] ) )

download_pkgs.start()
download_pkgs.finish()

